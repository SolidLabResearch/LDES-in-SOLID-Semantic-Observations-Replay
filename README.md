# LDES in SOLID Semantic Observations Replay

## Name
LDES in SOLID Semantic Observations Replay

## Description
Data streams are becoming omnipresent and are a crucial component in many use cases. Storing streams in a low-cost file-based web environment could be done using Linked Data Event Streams (LDES). However, pushing large volumes of high volatile data into a SOLID-based LDES is still not possible due to the way current solution do the partitioning of the data, i.e. after all data has been retrieved, instead of in a streaming fashion. The former crashes the Solid server due to the high load on the server when repartitioning large amounts of data.
Data from the DAHCC dataset will be used for purpose, which contains data streams describing the behaviour of various patients. The streams contain over 100.000 events.

As many applications are using data streams, a way to easily replay captured streams is necessary for demoing purposes, scalability testing or data recovery. Replaying allows to mimic the real-time behaviour of data streams, even though the data is historical data. A replayer is a crucial component to showcase how our solutions can process live data streams and how they can handle different data rates.
The DAHCC dataset will be used as example to replay the data.
## Installation
### engine: 

1. Run a LDES enables CSS, capable of processing LDES metadata e.g.  `docker run --rm -p 3000:3000 -it css:latest -c config/default.json`
2. Edit the properties file in the `engine/src/config` foler: `replay_properties.json` - sample datasets can be found in here: https://dahcc.idlab.ugent.be/dataset.html
3. Install the required dependencies: `npm i`
4. Start the engine with the command in the engine root folder: `npm start`

The output should look similar to the following:

`> challenge-16---replay---backend---typescript@1.0.0 start`

`> tsc && node dist/app.js`

`{port: [Getter], loglevel: [Getter], logname: [Getter], datasetFolders: [Getter], credentialsFileName: [Getter], lilURL: [Getter], treePath: [Getter], chunkSize: [Getter], bucketSize: [Getter], targetResourceSize: [Getter], default: {port: '3001', loglevel: 'info', logname: 'WEB API', datasetFolders: C:\\nextcloud\\development\\challenge16-replay\\main\\Challenge 16 - Replay - Backend - Typescript\\data', credentialsFileName: null, lilURL: 'http://localhost:3000/test/', treePath: 'https://saref.etsi.org/core/hasTimestamp', chunkSize: 10, bucketSize: 10, targetResourceSize: 1024}}`

`2022-12-08T14:58:54.612Z [WEB API] info: Express is listening at http://localhost:3001`

REMARK: Should you receive following error message

`src/algorithms/Naive.ts:66:51 - error TS2345: Argument of type 'import("C:/temp/CLEAN3/ldes-in-solid-semantic-observations-replay/engine/node_modules/@inrupt/solid-client-authn-node/dist/Session").Session' is not assignable to parameter of type 'import("C:/temp/CLEAN3/ldes-in-solid-semantic-observations-replay/engine/node_modules/@treecg/versionawareldesinldp/node_modules/@inrupt/solid-client-authn-node/dist/Session").Session'.`
` Types have separate declarations of a private property 'clientAuthentication'.`

`66     const comm = session ? new SolidCommunication(session) : new LDPCommunication();`

Please delete the following folder: `node_modules\@treecg\versionawareldesinldp\node_modules\@inrupt` This is due to conflicting dependencies and should be resolved onse the versionawareldesinldp package has been refactored.

### webapp: 

1. Install the required dependencies: `npm i`
2. Start the webapp in Vue.js with the command in the webapp root folder: `npm run dev`

The output should look similar to the following:

`> challenge-16---replay@0.0.0 dev`

`> vite`

`  VITE v3.0.9  ready in 6083 ms`

` ➜  Local:   http://127.0.0.1:5173/`

` ➜  Network: use --host to expose`

2. The webapp can then be accessed using your browser by navigating to the above mentioned URL. This should guide you to the front-end, similar to the picture below:

![Alt](/MainScreen.png "MainScreen")

3. Select the required dataset to be replayed into an LDES in Solid pod, e.g. `dataset_participant1_100obs` which holds 100 observations from the DAHCC dataset generated by Patricipant1.
4. Click `Load Selected Dataset`
5. Wait for the dataset to be loaded into the engine. The webpage should change into something like in the picture below:

![Alt](/DatasetLoaded.png "Dataset Loaded")

6. Sort the observation per timestamp by clicking on the `Sort observation subjects` button (a status indicator for the progress of the sorting is not yet provided)
7. In case desired, a sample of the loaded subjects can be retrieved by clicking on the `Get observation results` button. You should get something similar to:

![Alt](/ObservationSample.png "Observation Sample")

8. Now you can submit the first observation to be replayed into the LDES in Solid pod, by clicking on the `Submit next observation` button. You should see the `Current pointer position` variable at the bottom of the screen increase by one. This indicates that the observation was submitted, and that the pointer in the by timestamp sorted replay dataset is moved forward.

9. This can be repeated, resulting in more observation being submitted and resulting in the pointer ever moving forward

10. Alternatively, the remainder of the dataset can be replayed in one go from the current pointer onwards until the end of the dataset, by clicking on `Submit remaining observations`

11. Mind that replay progress can always be checked directly in the CSS as well, e.g. by surfing to `http://localhost:3000/test/`

## Usage
A streaming SOLID-based LDES connector that can partition the data in a streaming fashion, i.e. when retrieving the data instead of needing to wait till the whole dataset is received.

The replayer should be able to read a number of files, and stream out the events described in each file. To facility performance and scalability testing, the rates and number of streams should be configurable.

## Support
Stijn.Verstichel@UGent.be

## Roadmap
A connector is required that investigates the LDES, computes the need for a new bucket and adds the events of the stream either to a new bucket or to an existing one based on the timestamps of these events. In more detail the following functionality is required:
- [x] functionality to connect to and RDF stream, either pull or push based,
- [x] investigation of the current LDES, i.e. do new buckets need to be created,
- [x] if new buckets are required, the LDES itself needs to be updated,
- [x] new events are added to the right bucket, and
- [x] the bucket size should be parameterized.

The replayer should be a library that allows to:

- [x] read multiple files or directory,
- [x] stepwise replay,
- [x] bulk replay until the end of the dataset,
- [ ] stream out the result, possible using multiple output streams (configurable),
- [ ] configure the replay frequency, i.e. how fast the events should be streamed out,
- [ ] configure burst in the replay frequency, i.e. a certain times the event rate rises drastically to mimic high load/usage periods,
- [ ] assign new timestamps to the replayed events(the property should be configurable),
- [ ] configure the event shape (to know which triples belong to each event), and
- [ ] optionally first map raw data, e.g. CSV, to RDF



